library(caret)
vowel.test$y <- as.factor(vowel.test$y)
vowel.train$y <- as.factor(vowel.train$y)
set.seed(33833)
model <- randomForest(y ~ ., ntree, data = vowel.train)
library(randomForest)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
library(caret)
vowel.test$y <- as.factor(vowel.test$y)
vowel.train$y <- as.factor(vowel.train$y)
set.seed(33833)
model <- randomForest(y ~ ., ntree, data = vowel.train)
model <- randomForest(y ~ ., data = vowel.train)
varImp(model)
sorted(varImp(model))
# Getting the data
training_data_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
raw_df <- read.csv(training_data_url, header=T, sep = ",", na.strings = c("", "NA"))
# dim(raw_df) 160 variables
# Feature selection
# Removing variables with too much NA
na_threshold = 0.2
accepted_size <- nrow(raw_df)*na_threshold
na_values_per_column <- colSums(is.na(raw_df))
tidy_df <- raw_df[, na_values_per_column < accepted_size]
# dim(tidy_df) 60 variables
library(caret)
# Spliting the data in training and testing set
set.seed(0328)
inTrain = createDataPartition(tidy_df$classe, p = 3/4)[[1]]
training = tidy_df[ inTrain,]
testing = tidy_df[-inTrain,]
# Defining a general train control
tc <- trainControl(method = "cv", number = 5)
# Random Forest
model <- train(classe ~ .,data=training,method="rf",trControl= tc)
# Prediction
predictionRf <- predict(model, newdata = testing)
confusionMatrix(predictionRf, testing$classe)$overall[1]
predictionRf
model
# Getting the data
training_data_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
raw_df <- read.csv(training_data_url, header=T, sep = ",", na.strings = c("", "NA"))
# dim(raw_df) 160 variables
# Feature selection
# Removing variables with too much NA
na_threshold = 0.2
accepted_size <- nrow(raw_df)*na_threshold
na_values_per_column <- colSums(is.na(raw_df))
tidy_df <- raw_df[, na_values_per_column < accepted_size]
# dim(tidy_df) 60 variables
library(caret)
# Spliting the data in training and testing set
set.seed(0328)
inTrain = createDataPartition(tidy_df$classe, p = 3/4)[[1]]
training = tidy_df[ inTrain,]
testing = tidy_df[-inTrain,]
# Defining a general train control
tc <- trainControl(method = "cv", number = 5)
# Random Forest
model <- train(classe ~ .,data=training,method="rf",trControl= tc)
# Prediction
predictionRf <- predict(model, newdata = testing)
# Estimated error out of sample
sqrt(mean((predictionRf - testing$classe)^2))
confusionMatrix(predictionRf, testing$classe)
confusionMatrix(predictionRf, testing$classe)$overall[1]
colnames(tidy_df)
colnames(training)
colnames(testing)
validation_data_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
columns <- colnames(tidy_df)
columns <- columns[columns != "classe"]
columns
validation <- read.csv(validation_data_url, header=T, sep = ",", na.strings = c("", "NA"), col.names = columns)
raw_validation <- read.csv(validation_data_url, header=T, sep = ",", na.strings = c("", "NA"))
validation <- raw_validation[, columns]
dim(validation)
predictionRfValidation <- predict(model, newdata = validation)
predictionRfValidation
head(predictionRf)
hist(training$classe)
library(pandas) as pd
library(pandas)
install.packages("plyr")
install.packages("plyr")
install.packages("plyr")
install.packages("plyr")
library(plyr)
count(training$classe)
count(predictionRf)
count(predictionRfValidation)
model <- train(classe ~ .,data=training,method="rf",trControl= tc, importance = TRUE)
# Getting the data
training_data_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
raw_df <- read.csv(training_data_url, header=T, sep = ",", na.strings = c("", "NA"))
# dim(raw_df) 160 variables
# Feature selection
# Removing variables with too much NA
na_threshold = 0.2
accepted_size <- nrow(raw_df)*na_threshold
na_values_per_column <- colSums(is.na(raw_df))
tidy_df <- raw_df[, na_values_per_column < accepted_size]
# dim(tidy_df) 60 variables
library(plyr)
count(tidy_df) # The possibles values for the outcome variable are "almost" equally distributed
library(caret)
# Spliting the data in training and testing set
set.seed(31337)
inTrain = createDataPartition(tidy_df$classe, p = 3/4)[[1]]
training = tidy_df[ inTrain,]
testing = tidy_df[-inTrain,]
# Defining a general train control
tc <- trainControl(method = "cv", number = 3)
# Random Forest
model <- train(classe ~ .,data=training,method="rf",trControl= tc, importance = TRUE)
# Prediction
predictionRf <- predict(model, newdata = testing)
# Estimated accuracy out of the sample
confusionMatrix(predictionRf, testing$classe)$overall[1]
# Getting validation data
validation_data_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
columns <- colnames(training)
columns <- columns[columns != "classe"]
raw_validation <- read.csv(validation_data_url, header=T, sep = ",", na.strings = c("", "NA"))
validation <- raw_validation[, columns]
# Predict validation dataset
predictionRfValidation <- predict(model, newdata = validation)
predictionRfValidation
confusionMatrix(predictionRf, testing$classe)
colnames(validation)
dim(validation)
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
install.packages("doParallel")
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
install.packages("foreach", dependencies = T)
install.packages("foreach", dependencies = T)
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
tc <- trainControl(method = "cv", number = 3, allowParallel = TRUE)
model <- train(classe ~ .,data=training,method="gbm",trControl= tc)
stopCluster(cluster)
registerDoSEQ()
# Prediction
predictionRf <- predict(model, newdata = testing)
# Estimated accuracy out of the sample
confusionMatrix(predictionRf, testing$classe)$overall[1]
# Getting validation data
validation_data_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
columns <- colnames(training)
columns <- columns[columns != "classe"]
raw_validation <- read.csv(validation_data_url, header=T, sep = ",", na.strings = c("", "NA"))
validation <- raw_validation[, columns]
# Predict validation dataset
predictionRfValidation <- predict(model, newdata = validation)
predictionRfValidation
model <- train(classe ~ .,data=training,method="rf",trControl= tc, importance = TRUE)
model
head(validation)
predict(model, newdata = validation)
model <- train(classe ~ .,data=tidy_df,method="rf",trControl= tc, importance = TRUE)
prediction2 <- predict(model, newdata = raw_validation[raw_validation != "classe"])
prediction2 <- predict(model, newdata = raw_validation)
prediction2
model <- train(classe ~ .,data=tidy_df,method="rf")
install.packages("plotly")
install.packages("plotly")
install.packages("plotly")
install.packages("plotly")
library(plotly)
library(ggplot2)
library(plotly)
plot_ly(mtcars, x = wt, y = mpg, mode = "markers")
plot_ly(mtcars, x = ~wt, y = ~mpg, mode = "markers")
plot_ly(mtcars, x = ~wt, y = ~mpg, type = "scatter")
plot_ly(mtcars, x = ~wt, y = ~mpg, type = "scatter", mode = "markers")
plot_ly(mtcars, x = ~wt, y = ~mpg, type = "scatter", mode = "markers", color = ~factor(cyl))
plot_ly(mtcars, x = ~wt, y = ~mpg, type = "scatter", mode = "markers", color = as.factor(cyl))
plot_ly(mtcars, x = ~wt, y = ~mpg, type = "scatter", mode = "markers", color = ~disp)
plot_ly(mtcars, x = ~wt, y = ~mpg, type = "scatter",
mode = "markers", color = ~factor(cyl), size = ~hp)
set.seed(2016-07-21)
temp <- rnorm(100, mean = 30, sd = 5)
pressue <- rnorm(100)
dtime <- 1:100
plot_ly(x = ~temp, y = ~pressue, z = ~dtime,
type = "scatter3d", color = ~temp)
set.seed(2016-07-21)
temp <- rnorm(100, mean = 30, sd = 5)
pressue <- rnorm(100)
dtime <- 1:100
plot_ly(x = ~temp, y = ~pressue, z = ~dtime,
mode = "markers", type = "scatter3d", color = ~temp)
data("airmiles")
plot_ly(x = ~time(airmiles), y = ~airmiles, type = "scatter", mode = "lines")
library(plotly)
library(tidyr)
library(dplyr)
data("EuStockMarkets")
stocks <- as.data.frame(EuStockMarkets) %>%
gather(index, price) %>%
mutate(time = rep(time(EuStockMarkets), 4))
plot_ly(stocks, x = ~time, y = ~price, color = ~index, type = "scatter", mode = "lines")
plot_ly(x = ~precip, type = "histogram")
plot_ly(iris, y = ~Petal.Length, color = ~Species, type = "box")
terrain1 <- matrix(rnorm(100*100), nrow = 100, ncol = 100)
plot_ly(z = ~terrain1, type = "heatmap")
terrain2 <- matrix(sort(rnorm(100*100)), nrow = 100, ncol = 100)
plot_ly(z = ~terrain2, type = "surface")
# Create data frame
state_pop <- data.frame(State = state.abb, Pop = as.vector(state.x77[,1]))
# Create hover text
state_pop$hover <- with(state_pop, paste(State, '<br>', "Population:", Pop))
# Make state borders white
borders <- list(color = toRGB("red"))
# Set up some mapping options
map_options <- list(
scope = 'usa',
projection = list(type = 'albers usa'),
showlakes = TRUE,
lakecolor = toRGB('white')
)
# Create data frame
state_pop <- data.frame(State = state.abb, Pop = as.vector(state.x77[,1]))
# Create hover text
state_pop$hover <- with(state_pop, paste(State, '<br>', "Population:", Pop))
# Make state borders white
borders <- list(color = toRGB("red"))
# Set up some mapping options
map_options <- list(
scope = 'usa',
projection = list(type = 'albers usa'),
showlakes = TRUE,
lakecolor = toRGB('white')
)
plot_ly(z = ~state_pop$Pop, text = ~state_pop$hover, locations = ~state_pop$State,
type = 'choropleth', locationmode = 'USA-states',
color = state_pop$Pop, colors = 'Blues', marker = list(line = borders)) %>%
layout(title = 'US Population in 1975', geo = map_options)
set.seed(101)
d <- diamonds[sample(nrow(diamonds), 1000),]
geom_smooth(aes(colour = clarity, fill=clarity)) + facet_wrap(~ clarity)
gg <- ggplotly(p)
gg
library(ggplot2)
set.seed(101)
d <- diamonds[sample(nrow(diamonds), 1000),]
geom_smooth(aes(colour = clarity, fill=clarity)) + facet_wrap(~ clarity)
gg <- ggplotly(p)
gg
library(plotly)
library(ggplot2)
library(tidyquant)
library(tidyr)
library(dplyr)
set.seed(101)
d <- diamonds[sample(nrow(diamonds), 1000),]
geom_smooth(aes(colour = clarity, fill=clarity)) + facet_wrap(~ clarity)
gg <- ggplotly(p)
gg
install.packages("leaflet")
install.packages("leaflet")
# Downloading and uncompressing a dataset of cities in Colombia
url_cities_co = 'http://download.geonames.org/export/dump/CO.zip'
download(url_cities_co, dest='CO.zip', mode='wb')
unzip ('CO.zip', exdir = './raw_data/')
# Reading the dataset of cities
cities_colombia <- read.csv(file='./raw_data/CO.txt', header=FALSE, sep='\t')
# Columns values are explained in ./raw_data/readme.txt
colnames(cities_colombia) <- c('geonameid', 'name', 'asciiname', 'alternatenames',
'latitude', 'longitude', 'feature_class', 'feature_code',
'country_code', 'cc2', 'admin1_code', 'admin2_code',
'admin3_code', 'admin4_code', 'population',
'elevation', 'dem', 'timezone', 'modification_date')
knitr::opts_chunk$set(echo = TRUE)
url_cities_co = 'http://download.geonames.org/export/dump/CO.zip'
download.file(url_cities_co, destfile='CO.zip', mode='wb')
download.file(url_cities_co, destfile='./raw_data/CO.zip', mode='wb')
unzip ('./raw_data/CO.zip', exdir = './raw_data/')
cities_colombia <- read.csv(file='./raw_data/CO.txt', header=FALSE, sep='\t')
colnames(cities_colombia) <- c('geonameid', 'name', 'asciiname', 'alternatenames',
'latitude', 'longitude', 'feature_class', 'feature_code',
'country_code', 'cc2', 'admin1_code', 'admin2_code',
'admin3_code', 'admin4_code', 'population',
'elevation', 'dem', 'timezone', 'modification_date')
head(cities_colombia)
len(cities_colombia)
length(cities_colombia)
numcols(cities_colombia)
ncol(cities_colombia)
nrows(cities_colombia)
nrow(cities_colombia)
places_colombia <- read.csv(file='./raw_data/CO.txt', header=FALSE, sep='\t')
# Columns values are explained in ./raw_data/readme.txt
colnames(cities_colombia) <- c('geonameid', 'name', 'asciiname', 'alternatenames',
'latitude', 'longitude', 'feature_class', 'feature_code',
'country_code', 'cc2', 'admin1_code', 'admin2_code',
'admin3_code', 'admin4_code', 'population',
'elevation', 'dem', 'timezone', 'modification_date')
# nrow(places_colombia)
cities_colombia <- subset(places_colombia, feature_code == 'P')
places_colombia <- read.csv(file='./raw_data/CO.txt', header=FALSE, sep='\t')
setwd()
setwd("~/Desktop/Repositories/developing_data_products/week2/hw1")
places_colombia <- read.csv(file='./raw_data/CO.txt', header=FALSE, sep='\t')
colnames(cities_colombia) <- c('geonameid', 'name', 'asciiname', 'alternatenames',
'latitude', 'longitude', 'feature_class', 'feature_code',
'country_code', 'cc2', 'admin1_code', 'admin2_code',
'admin3_code', 'admin4_code', 'population',
'elevation', 'dem', 'timezone', 'modification_date')
cities_colombia <- subset(places_colombia, feature_code == 'P')
cities_colombia <- subset(places_colombia, ~feature_code == 'P')
columns <- c('name', 'latitude', 'longitude', 'admin1_code')
cities_colombia <- subset(places_colombia, feature_code == 'P', select=columns)
nrow(places_colombia)
head(places_colombia)
colnames(places_colombia) <- c('geonameid', 'name', 'asciiname', 'alternatenames',
'latitude', 'longitude', 'feature_class', 'feature_code',
'country_code', 'cc2', 'admin1_code', 'admin2_code',
'admin3_code', 'admin4_code', 'population',
'elevation', 'dem', 'timezone', 'modification_date')
cities_colombia <- subset(places_colombia, feature_code == 'P', select=columns)
nrow(cities_colombia)
ncol(cities_colombia)
levels(places_colombia$feature_code)
levels(places_colombia$feature_class)
levels(factor(places_colombia$feature_class))
cities_colombia <- subset(places_colombia, feature_class == 'P', select=columns)
cities_colombia <- unique(cities_colombia)
cities_colombia <- subset(places_colombia, feature_class == 'P', select=columns)
cities_colombia <- unique(cities_colombia)
cities_colombia$admin1_code = paste(country_prefix, cities_colombia$admin1_code, sep='.')
country_prefix = 'CO'
cities_colombia$admin1_code = paste(country_prefix, cities_colombia$admin1_code, sep='.')
head(cities_colombia)
cities_colombia <- subset(places_colombia, feature_class == 'P', select=columns)
cities_colombia$admin1_code = sprintf("%02d", as.numeric(cities_colombia$admin1_code))
cities_colombia$admin1_code = paste(country_prefix, cities_colombia$admin1_code, sep='.')
head(cities_colombia)
url_regions = 'http://download.geonames.org/export/dump/admin1CodesASCII.txt'
download.file(url_regions, destfile='./raw_data/admin1CodesASCII.txt', mode='wb')
regions_world <- read.csv(file='./raw_data/admin1CodesASCII.txt', header=FALSE, sep='\t')
colnames(regions_world) <- c('code', 'name', 'nameAscii', 'geonameid')
head(regions_world)
cities_colombia <- merge(cities_colombia, regions_world, by='admin1_code')
cities_colombia <- subset(places_colombia, feature_class == 'P', select=columns)
cities_colombia$admin1_code = sprintf("%02d", as.numeric(cities_colombia$admin1_code))
cities_colombia$admin1_code = paste(country_prefix, cities_colombia$admin1_code, sep='.')
cities_colombia2 <- merge(x=cities_colombia, y=regions_world, x.by='admin1_code', y.by='code')
head(cities_colombia2)
cities_colombia2 <- merge(x=cities_colombia, y=regions_world, by.x='admin1_code', by.y='code')
cities_colombia <- merge(x=cities_colombia, y=regions_world, by.x='admin1_code', by.y='code')
head(cities_colombia)
cities_colombia2 <- cities_colombia[columns]
columns <- c('name.x', 'latitude', 'longitude', 'name.y')
cities_colombia2 <- cities_colombia[columns]
cities_colombia <- cities_colombia[columns]
colnames(cities_colombia) <- c('city', 'latitude', 'longitude', 'region')
head(cities_colombia)
write.csv(cities_colombia, file = "./intermediate_data/cities_colombia.csv", na="")
write.csv(cities_colombia, file = "./intermediate_data/cities_colombia.csv", row.names=FALSE, na="")
cities_colombia <- subset(places_colombia, feature_class == 'P', select=columns)
columns <- c('name', 'latitude', 'longitude', 'admin1_code')
country_prefix = 'CO'
cities_colombia <- subset(places_colombia, feature_class == 'P', select=columns)
cities_colombia$admin1_code = sprintf("%02d", as.numeric(cities_colombia$admin1_code))
cities_colombia$admin1_code = paste(country_prefix, cities_colombia$admin1_code, sep='.')
# Downloading and reading data about regions in the world
url_regions = 'http://download.geonames.org/export/dump/admin1CodesASCII.txt'
download.file(url_regions, destfile='./raw_data/admin1CodesASCII.txt', mode='wb')
regions_world <- read.csv(file='./raw_data/admin1CodesASCII.txt', header=FALSE, sep='\t')
colnames(regions_world) <- c('code', 'name', 'nameAscii', 'geonameid')
# Adding region column to cities in Colombia
cities_colombia <- merge(x=cities_colombia, y=regions_world, by.x='admin1_code', by.y='code')
columns <- c('name.x', 'latitude', 'longitude', 'nameAscii')
cities_colombia <- cities_colombia[columns]
colnames(cities_colombia) <- c('city', 'latitude', 'longitude', 'region')
cities_colombia <- tolower(cities_colombia)
write.csv(cities_colombia, file = "./intermediate_data/cities_colombia.csv", row.names=FALSE, na="")
columns <- c('name', 'latitude', 'longitude', 'admin1_code')
country_prefix = 'CO'
cities_colombia <- subset(places_colombia, feature_class == 'P', select=columns)
cities_colombia$admin1_code = sprintf("%02d", as.numeric(cities_colombia$admin1_code))
cities_colombia$admin1_code = paste(country_prefix, cities_colombia$admin1_code, sep='.')
# Downloading and reading data about regions in the world
url_regions = 'http://download.geonames.org/export/dump/admin1CodesASCII.txt'
download.file(url_regions, destfile='./raw_data/admin1CodesASCII.txt', mode='wb')
regions_world <- read.csv(file='./raw_data/admin1CodesASCII.txt', header=FALSE, sep='\t')
colnames(regions_world) <- c('code', 'name', 'nameAscii', 'geonameid')
# Adding region column to cities in Colombia
cities_colombia <- merge(x=cities_colombia, y=regions_world, by.x='admin1_code', by.y='code')
columns <- c('name.x', 'latitude', 'longitude', 'nameAscii')
cities_colombia <- cities_colombia[columns]
colnames(cities_colombia) <- c('city', 'latitude', 'longitude', 'region')
cities_colombia$city <- tolower(cities_colombia$city)
cities_colombia$region <- tolower(cities_colombia$region)
write.csv(cities_colombia, file = "./intermediate_data/cities_colombia.csv", row.names=FALSE, na="")
url_disasters= 'https://www.datos.gov.co/api/views/xjv9-mim9/rows.csv?accessType=DOWNLOAD'
download.file(url_disasters, destfile='./raw_data/disasters_co.csv', mode='wb')
disasters_co <- read.csv(file='./raw_data/disasters_co.csv', header=FALSE, sep='\t')
disasters_co <- read.csv(file='./raw_data/disasters_co.csv', header=TRUE, sep=',')
head(disasters_co)
summary(disasters_co)
columns <- c('fecha', 'departamento', 'municipio', 'evento',
'muertos', 'heridos', 'desapa', 'personas', 'familias')
disasters_co <- disasters_co[columns]
colnames(disasters_co) <- c('date', 'region', 'city', 'event', 'deaths',
'injured', 'missing_people', 'people_affected',
'families_affected')
disasters_co$date <- as.POSIXct(disasters_co$date, "%d/%m/%Y %h:%M:%S")
disasters_co$date <- as.POSIXct(disasters_co$date, "%d/%m/%Y %h:%M:%S", tz = "America/Bogota")
OlsonNames()
disasters_co$date <- as.POSIXct(disasters_co$date, "%d/%m/%Y %h:%M:%S", tz = "America/Bogota")
head(disasters_co)
url_disasters= 'https://www.datos.gov.co/api/views/xjv9-mim9/rows.csv?accessType=DOWNLOAD'
download.file(url_disasters, destfile='./raw_data/disasters_co.csv', mode='wb')
disasters_co <- read.csv(file='./raw_data/disasters_co.csv', header=TRUE, sep=',')
columns <- c('fecha', 'departamento', 'municipio', 'evento',
'muertos', 'heridos', 'desapa', 'personas', 'familias')
disasters_co <- disasters_co[columns]
colnames(disasters_co) <- c('date', 'region', 'city', 'event', 'deaths',
'injured', 'missing_people', 'people_affected',
'families_affected')
disasters_co$date <- as.POSIXct(disasters_co$date, "%d/%m/%Y %h:%M:%S", tz = "America/Bogota")
head(disasters_co)
disasters_co <- read.csv(file='./raw_data/disasters_co.csv', header=TRUE, sep=',')
columns <- c('fecha', 'departamento', 'municipio', 'evento',
'muertos', 'heridos', 'desapa', 'personas', 'familias')
disasters_co <- disasters_co[columns]
colnames(disasters_co) <- c('date', 'region', 'city', 'event', 'deaths',
'injured', 'missing_people', 'people_affected',
'families_affected')
disasters_co$date <- as.POSIXct(disasters_co$date, "%d/%m/%Y %I:%M:%S %p", tz = "America/Bogota")
head(disasters_co)
disasters_co <- read.csv(file='./raw_data/disasters_co.csv', header=TRUE, sep=',')
columns <- c('fecha', 'departamento', 'municipio', 'evento',
'muertos', 'heridos', 'desapa', 'personas', 'familias')
disasters_co <- disasters_co[columns]
colnames(disasters_co) <- c('date', 'region', 'city', 'event', 'deaths',
'injured', 'missing_people', 'people_affected',
'families_affected')
disasters_co$date <- as.POSIXct(disasters_co$date, "%m/%d/%Y %I:%M:%S %p", tz = "America/Bogota")
head(disasters_co)
disasters_co$year <- format(disasters_co$date,"%Y")
head(disasters_co)
levels(disasters_co$event)
levels(disasters_co$year)
levels(factor(disasters_co$year))
summary(disasters_co)
columns <- c('date', 'region', 'city')
flood_co <- subset(disasters_co, event=='INUNDACION', select=columns)
head(flood_co)
flood_co$city <- tolower(flood_co$city)
flood_co$region <- tolower(flood_co$region)
flood_co2 <- merge(x=flood_co, y=cities_colombia, by=c('city', 'region'), all.x=TRUE)
cities_colombia <- unique(cities_colombia)
flood_co2 <- merge(x=flood_co, y=cities_colombia, by=c('city', 'region'), all.x=TRUE)
summary(flood_co2)
levels(flood_co2$longitude)
head(flood_co2)
tail(flood_co2)
sum(is.na(flood_co2$latitude))
flood_co2[is.na(flood_co2$latitude)]
floods_co <- floods_co[is.na(flood_co2$latitude),]
floods_co <- flood_co2[is.na(flood_co2$latitude),]
floods_co
head(floods_co)
floods_co <- unique(floods_co[c('city', 'region')])
head(floods_co)
floods_co2 <- unique(floods_co[c('region')])
head(floods_co2)
regions <- as.vector(unique(cities_colombia$region))
install.packages('fuzzywuzzyR')
library(fuzzywuzzyR)
init_proc = FuzzUtils$new()      # initialization of FuzzUtils class to choose a processor
PROC = init_proc$Full_process    # processor-method
init_scor = FuzzMatcher$new()    # initialization of the scorer class
SCOR = init_scor$WRATIO          # choosen scorer function
floods_co$latitude <- 0
floods_co$longitude <- 0
floods_co$fuzzy_score <- 1
floods_co <- subset(disasters_co, event=='INUNDACION', select=columns)
floods_co$city <- tolower(flood_co$city)
floods_co$region <- tolower(flood_co$region)
regions <- as.vector(unique(cities_colombia$region))
best_region <- init$ExtractOne(string = region_flood, sequence_strings = regions, processor = PROC,
scorer = SCOR, score_cutoff = 0L)
init <- FuzzExtract$new()        # Initialization of the FuzzExtract class
best_region <- init$ExtractOne(string = 'Nariño', sequence_strings = regions, processor = PROC,
scorer = SCOR, score_cutoff = 0L)
init$ExtractOne(string = 'Nariño', sequence_strings = regions, processor = PROC,
scorer = SCOR, score_cutoff = 0L)
reticulate::py_discover_config(required_module="fuzzywuzzy")
reticulate::py_discover_config(required_module="fuzzywuzzy")
reticulate::py_discover_config(required_module = 'Levenshtein')
reticulate::py_discover_config(required_module = 'Levenshtein')
reticulate::py_discover_config(required_module = 'difflib')
best_region <- init$ExtractOne(string = 'Nariño', sequence_strings = regions, processor = PROC,
scorer = SCOR, score_cutoff = 0L)
reticulate::py_discover_config()
check_availability()
