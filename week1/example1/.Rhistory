pnorm(-1)
qnorm(0.95, 1100, 75)
n <- 100
m <- cumsum(rnorm(n, 1100, 75))/(1:n)
m
plot(m)
pbinom(3, size = 5, prob = 0.5, lower.tail = FALSE)
n <- 100
pnorm(16, mean = 15, sd = 1) - pnorm(14, mean = 15, sd = 1)
cumsum(rnorm(1000, .5, sqrt(0.12)))/(1:1000)
ppois(10, 5*3)
library(caret)
install.packages(caret)
install.packages(caret)
install.packages("caret")
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
install.packages("AppliedPredictionModels")
install.packages("AppliedPredictionModeling")
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
dim(training)
summary(training)
install.packages(hmisc)
install.packages("Hmisc")
install.packages("Hmisc")
library(Hmisc)
cutCement <- cut2(training$Cement, g=3)
plot(CompressiveStrength, colour=cutCement, data=training)
install.packages("ggplot2")
install.packages("ggplot2")
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
dim(training) # 774 9
summary(training)
library(Hmisc)
cutCement <- cut2(training$Cement, g=3)
plot(CompressiveStrength, colour=cutCement, data=training)
install.packages("ggplot2")
update.packages(ask = FALSE)
system('defaults write org.R-project.R force.LANG en_US.UTF-8')
update.packages(ask = FALSE)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
dim(training) # 774 9
summary(training)
library(Hmisc)
cutCement <- cut2(training$Cement, g=3)
plot(CompressiveStrength, colour=cutCement, data=training)
old.packages()
install.packages("Hmisc")
old.packages()
defaults write org.R-project.R force.LANG en_US.UTF-8
System("defaults write org.R-project.R force.LANG en_US.UTF-8")
system('defaults write org.R-project.R force.LANG en_US.UTF-8')
install.packages("AppliedPredictiveModeling")
install.packages("ElementStatLearn")
install.packages("ElemStatLearn")
install.packages("pgmm")
install.packages("rpart")
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
install.packages("caret")
install.packages("caret")
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
install.packages("caret", dependencies = TRUE)
install.packages("caret", dependencies = TRUE)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
dim(segmentationOriginal)
# I'm assuming a partition 70% for training and 30% for testing
trainIndex = createDataPartition(segmentationOriginal$Case, p = 0.70,list=FALSE)
training <- segmentationOriginal[trainIndex,]
testing <- segmentationOriginal[-trainIndex,]
set.seed(125)
modFit <- train(Case ~ ., method = "rpart", data = training)
plot(modFit$finalModel, uniform=TRUE, main = "Classification Tree")
text(modFit$finalModel, use.n = TRUE, all = TRUE, cex = 0.8)
install.packages("rattle")
install.packages("rattle", dependencies = TRUE)
library(rattle)
fancyRpartPlot(modFit$finalModel)
install.packages("RGtk2")
install.packages("rattle")
install.packages("rattle", dependencies = TRUE, type = "source")
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("ggplot2")
install.packages("magrittr")
install.packages("magrittr")
install.packages("magrittr")
install.packages("RGtk2")
install.packages("rattle", dependencies=TRUE, repos='http://cran.rstudio.com/')
install.packages("rattle", dependencies=TRUE, repos='http://cran.rstudio.com/')
library(rattle)
version
library(pgmm)
data(olive)
library(pgmm)
data(olive)
olive = olive[,-1]
dim(olive)
summary(olive)
newdata = as.data.frame(t(colMeans(olive)))
head(newdata)
install.packages("rpart.plot")
library(pgmm)
data(olive)
olive = olive[,-1]
newdata = as.data.frame(t(colMeans(olive)))
# I'm assuming a partition 70% for training and 30% for testing
trainIndex = createDataPartition(olive$Area, p = 0.70,list=FALSE)
training <- segmentationOriginal[trainIndex,]
testing <- segmentationOriginal[-trainIndex,]
modFit <- train(Case ~ ., method = "rpart", data = training)
predict(modFit, newdata = newdata)
library(caret)
library(pgmm)
data(olive)
olive = olive[,-1]
newdata = as.data.frame(t(colMeans(olive)))
# I'm assuming a partition 70% for training and 30% for testing
trainIndex = createDataPartition(olive$Area, p = 0.70,list=FALSE)
training <- segmentationOriginal[trainIndex,]
testing <- segmentationOriginal[-trainIndex,]
modFit <- train(Case ~ ., method = "rpart", data = training)
predict(modFit, newdata = newdata)
library(pgmm)
data(olive)
library(caret)
olive = olive[,-1]
newdata = as.data.frame(t(colMeans(olive)))
# I'm assuming a partition 70% for training and 30% for testing
trainIndex = createDataPartition(olive$Area, p = 0.70,list=FALSE)
training <- olive[trainIndex,]
testing <- olive[-trainIndex,]
modFit <- train(Case ~ ., method = "rpart", data = training)
predict(modFit, newdata = newdata)
question3 <- funtion() {
library(pgmm)
data(olive)
library(caret)
olive = olive[,-1]
newdata = as.data.frame(t(colMeans(olive)))
# I'm assuming a partition 70% for training and 30% for testing
trainIndex = createDataPartition(olive$Area, p = 0.70,list=FALSE)
training <- olive[trainIndex,]
testing <- olive[-trainIndex,]
modFit <- train(Area ~ ., method = "rpart", data = training)
predict(modFit, newdata = newdata)
}
library(pgmm)
data(olive)
library(caret)
olive = olive[,-1]
newdata = as.data.frame(t(colMeans(olive)))
# I'm assuming a partition 70% for training and 30% for testing
trainIndex = createDataPartition(olive$Area, p = 0.70,list=FALSE)
training <- olive[trainIndex,]
testing <- olive[-trainIndex,]
modFit <- train(Area ~ ., method = "rpart", data = training)
predict(modFit, newdata = newdata)
summary(olive)
n4 <- function() {
library(caret)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
}
library(caret)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
dim(SAheart)
summary(SAheart)
library(caret)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method="glm", family="binomial")
prediction <- predict(modFit, newdata = testSA)
values <- testSA[, c("chd")]
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method="glm", family="binomial", data = trainSA)
prediction <- predict(modFit, newdata = testSA)
values <- testSA[, c("chd")]
Summary(trainSA)
Summary(train)
summary(trainSA)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass()
missClass(values, prediction)
trainPrediction <- predict(modFit, newdata = trainSA)
trainValues <- trainSA$chd
missClass(trainValues, trainPrediction)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
library(caret)
vowel.test$y <- as.factor(vowel.test$y)
vowel.train$y <- as.factor(vowel.train$y)
set.seed(33833)
model <- randomForest(y ~ ., ntree, data = vowel.train)
library(randomForest)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
library(caret)
vowel.test$y <- as.factor(vowel.test$y)
vowel.train$y <- as.factor(vowel.train$y)
set.seed(33833)
model <- randomForest(y ~ ., ntree, data = vowel.train)
model <- randomForest(y ~ ., data = vowel.train)
varImp(model)
sorted(varImp(model))
# Getting the data
training_data_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
raw_df <- read.csv(training_data_url, header=T, sep = ",", na.strings = c("", "NA"))
# dim(raw_df) 160 variables
# Feature selection
# Removing variables with too much NA
na_threshold = 0.2
accepted_size <- nrow(raw_df)*na_threshold
na_values_per_column <- colSums(is.na(raw_df))
tidy_df <- raw_df[, na_values_per_column < accepted_size]
# dim(tidy_df) 60 variables
library(caret)
# Spliting the data in training and testing set
set.seed(0328)
inTrain = createDataPartition(tidy_df$classe, p = 3/4)[[1]]
training = tidy_df[ inTrain,]
testing = tidy_df[-inTrain,]
# Defining a general train control
tc <- trainControl(method = "cv", number = 5)
# Random Forest
model <- train(classe ~ .,data=training,method="rf",trControl= tc)
# Prediction
predictionRf <- predict(model, newdata = testing)
confusionMatrix(predictionRf, testing$classe)$overall[1]
predictionRf
model
# Getting the data
training_data_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
raw_df <- read.csv(training_data_url, header=T, sep = ",", na.strings = c("", "NA"))
# dim(raw_df) 160 variables
# Feature selection
# Removing variables with too much NA
na_threshold = 0.2
accepted_size <- nrow(raw_df)*na_threshold
na_values_per_column <- colSums(is.na(raw_df))
tidy_df <- raw_df[, na_values_per_column < accepted_size]
# dim(tidy_df) 60 variables
library(caret)
# Spliting the data in training and testing set
set.seed(0328)
inTrain = createDataPartition(tidy_df$classe, p = 3/4)[[1]]
training = tidy_df[ inTrain,]
testing = tidy_df[-inTrain,]
# Defining a general train control
tc <- trainControl(method = "cv", number = 5)
# Random Forest
model <- train(classe ~ .,data=training,method="rf",trControl= tc)
# Prediction
predictionRf <- predict(model, newdata = testing)
# Estimated error out of sample
sqrt(mean((predictionRf - testing$classe)^2))
confusionMatrix(predictionRf, testing$classe)
confusionMatrix(predictionRf, testing$classe)$overall[1]
colnames(tidy_df)
colnames(training)
colnames(testing)
validation_data_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
columns <- colnames(tidy_df)
columns <- columns[columns != "classe"]
columns
validation <- read.csv(validation_data_url, header=T, sep = ",", na.strings = c("", "NA"), col.names = columns)
raw_validation <- read.csv(validation_data_url, header=T, sep = ",", na.strings = c("", "NA"))
validation <- raw_validation[, columns]
dim(validation)
predictionRfValidation <- predict(model, newdata = validation)
predictionRfValidation
head(predictionRf)
hist(training$classe)
library(pandas) as pd
library(pandas)
install.packages("plyr")
install.packages("plyr")
install.packages("plyr")
install.packages("plyr")
library(plyr)
count(training$classe)
count(predictionRf)
count(predictionRfValidation)
model <- train(classe ~ .,data=training,method="rf",trControl= tc, importance = TRUE)
# Getting the data
training_data_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
raw_df <- read.csv(training_data_url, header=T, sep = ",", na.strings = c("", "NA"))
# dim(raw_df) 160 variables
# Feature selection
# Removing variables with too much NA
na_threshold = 0.2
accepted_size <- nrow(raw_df)*na_threshold
na_values_per_column <- colSums(is.na(raw_df))
tidy_df <- raw_df[, na_values_per_column < accepted_size]
# dim(tidy_df) 60 variables
library(plyr)
count(tidy_df) # The possibles values for the outcome variable are "almost" equally distributed
library(caret)
# Spliting the data in training and testing set
set.seed(31337)
inTrain = createDataPartition(tidy_df$classe, p = 3/4)[[1]]
training = tidy_df[ inTrain,]
testing = tidy_df[-inTrain,]
# Defining a general train control
tc <- trainControl(method = "cv", number = 3)
# Random Forest
model <- train(classe ~ .,data=training,method="rf",trControl= tc, importance = TRUE)
# Prediction
predictionRf <- predict(model, newdata = testing)
# Estimated accuracy out of the sample
confusionMatrix(predictionRf, testing$classe)$overall[1]
# Getting validation data
validation_data_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
columns <- colnames(training)
columns <- columns[columns != "classe"]
raw_validation <- read.csv(validation_data_url, header=T, sep = ",", na.strings = c("", "NA"))
validation <- raw_validation[, columns]
# Predict validation dataset
predictionRfValidation <- predict(model, newdata = validation)
predictionRfValidation
confusionMatrix(predictionRf, testing$classe)
colnames(validation)
dim(validation)
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
install.packages("doParallel")
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
install.packages("foreach", dependencies = T)
install.packages("foreach", dependencies = T)
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
tc <- trainControl(method = "cv", number = 3, allowParallel = TRUE)
model <- train(classe ~ .,data=training,method="gbm",trControl= tc)
stopCluster(cluster)
registerDoSEQ()
# Prediction
predictionRf <- predict(model, newdata = testing)
# Estimated accuracy out of the sample
confusionMatrix(predictionRf, testing$classe)$overall[1]
# Getting validation data
validation_data_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
columns <- colnames(training)
columns <- columns[columns != "classe"]
raw_validation <- read.csv(validation_data_url, header=T, sep = ",", na.strings = c("", "NA"))
validation <- raw_validation[, columns]
# Predict validation dataset
predictionRfValidation <- predict(model, newdata = validation)
predictionRfValidation
model <- train(classe ~ .,data=training,method="rf",trControl= tc, importance = TRUE)
model
head(validation)
predict(model, newdata = validation)
model <- train(classe ~ .,data=tidy_df,method="rf",trControl= tc, importance = TRUE)
prediction2 <- predict(model, newdata = raw_validation[raw_validation != "classe"])
prediction2 <- predict(model, newdata = raw_validation)
prediction2
model <- train(classe ~ .,data=tidy_df,method="rf")
install.packages("shiny")
shiny::runApp('Desktop/Repositories/developing_data_products/week1/example1')
runApp('Desktop/Repositories/developing_data_products/week1/example1')
setwd("~/Desktop/Repositories/developing_data_products/week1/example1")
runApp()
?builder
runApp()
runApp()
runApp('~/Desktop/Repositories/developing_data_products/week1/example2')
runApp('~/Desktop/Repositories/developing_data_products/week1/example2')
install.packages("miniUI")
install.packages("GoogleVis")
install.packages("googleVis")
suppressPackageStartupMessages(library(googleVis))
M <- gvisMotionChart(Fruits, "Fruit", "Year",
options=list(width=600, height=400))
print(M,"chart")
suppressPackageStartupMessages(library(googleVis))
M <- gvisMotionChart(Fruits, "Fruit", "Year",
options=list(width=600, height=400))
plot(M,"chart")
plot(M)
suppressPackageStartupMessages(library(googleVis))
M <- gvisMotionChart(Fruits, "Fruit", "Year",
options=list(width=600, height=400))
plot(M)
G <- gvisGeoChart(Exports, locationvar="Country",
colorvar="Profit",options=list(width=600, height=400))
plot(G)
